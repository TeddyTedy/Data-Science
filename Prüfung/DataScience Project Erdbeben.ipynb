{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research of earthquake\n",
    "###  Earthquake (Björn Bauer, 19.11.2021)\n",
    "\n",
    "#### Project Summary\n",
    "\n",
    "Grundsätzliches Ziel der Aufgabe ist das erforschen von Erdbeben-Daten der letzte 2 Jahre. Dabei sind die Hauptfragen, die durch die Analyse der\n",
    "Daten geklärt werden sollen:\n",
    "-          how likely it's that in places where there have already been earthquakes there will be more earthquakes? What would be the trend in 10 years?\n",
    "\n",
    "-          Whether there are special places where earthquakes occur certain strength piling. \n",
    "\n",
    "-          A correlation between values such as depth and strength (or other values related to strength)\n",
    "\n",
    "Ziel ist es, mögliche Muster in scheinbar zufälligen Ereignissen zu finden und falls möglich vorhersagen über die Zuunft zu treffen.\n",
    "\n",
    "-- 1.       Processing Data: please specify\n",
    "\n",
    "Steps to solve:\n",
    "\n",
    "1.1.  Perform EDA for the provided data \n",
    "\n",
    "a.       Perform EDA \n",
    "\n",
    "b.       Compute the Pearson correlation coefficient. Which conclusion can you make? \n",
    "\n",
    "1.2.  Linear regression \n",
    "\n",
    "a.                   Define linear function for your analysis. That is, f=ai+b, where a is the slope and b is the intercept. So find the best fit line using np.polyfit().\n",
    "\n",
    " \n",
    "\n",
    "b.                   Plot the data and the best fit line. Print out the slope and intercept. (Think: what are their units?)\n",
    "\n",
    "1.3.  How is it optimal?\n",
    "\n",
    "a.                   The function np.polyfit() that you used to get your regression parameters finds the optimal slope and intercept. It is optimizing the sum of the squares of the residuals, also known as RSS (for residual sum of squares). Plot the function that is being optimized, the RSS, versus the slope parameter a. To do this, fix the intercept to be what you found in the optimization. Then, plot the RSS vs. the slope. Where is it minimal? What does it mean for the research?\n",
    "\n",
    "1.4.  Pairs bootstrap or permutation? \n",
    "\n",
    "a.                   Perform pairs bootstrap or permutation to plot a histogram describing the estimate of the slope from the  data. Also report the 95% confidence interval of the slope.\n",
    "\n",
    "1.5. Plotting bootstrap or permutation regressions \n",
    "\n",
    "a.                   A nice way to visualize the variability we might expect in a linear regression is to plot the line you would get from each replicate of the slope and intercept. Do this for the first 100 of your replicates of the slope and intercept\n",
    "\n",
    "1.6.  Hypothesis test on Pearson correlation\n",
    "\n",
    "a.                   Formulate Hypothesis and Test it. Provide a conclusion for it\n",
    "\n",
    "1.7.  Provide the conclusion of your earthquake research. Is it possible to perfume here A/B Test? What it can show? What confidence intervals and p-values show us in regard to testing of Null-hypothesis?\n",
    "\n",
    "1.8. What shows us a Bonferroni test? Use: https://www.investopedia.com/terms/b/bonferroni-test.asp\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data (perform EDA, provide error handlings, unit tests, data quality tests, etc)\n",
    "* Step 3: Define the Data Model/ Null/Alternative Hypothesis\n",
    "* Step 4: Show the Model of  Data and any conclutions\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import statistics\n",
    "import warnings\n",
    "from sympy import S, symbols, printing\n",
    "#für Chi-Quadrat nötig\n",
    "#from scipy.stats import chi2_contingency\n",
    "#from scipy.stats import chi2\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "\n",
    "- Neben der gestellten Aufgabenstellung ist das Hauptziel die BEantwortung der Forschungsfragen und der Versuch Muster in einem normalerweise zufällig erscheinenden Verhalten zu finden\n",
    "- Durch gezielte Visualisierung sollen Muster erkannt werden\n",
    "- Endsolution soll im Idealfall Graphisch die Hauptfragen der Arbeit beantworten\n",
    "- Tool hierfür ist die Programmiersprache Python und die Module pandas, Seaborn, numpy, mathplotlip.pyplot\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? \n",
    "- Die Daten in diesem Projekt handeln von Erdbeben, die Tiefe, die Location, wann Sie aufgetreten sind, die Stärke\n",
    "- Die Grunddaten lassen keine Daten aus, die in Bezug auf die Analyse von Erbeben wichtig sind, außer Phsikalische Parameter wie Geschwindigkeit der Wellenfortbewegung und Art des Gesteins. Beides ist für die Fragestellung nicht von Bedeutung\n",
    "    - In dem Cleaning Step werden Daten die für die Analyse nicht benötigt werden entfernt\n",
    "- Quelle der Daten: https://www.usgs.gov/natural-hazards/earthquake-hazards/earthquakes\n",
    "    - Hier können verschiedene Daten abgefragt werden, in diesem Fall geht es um die Daten der letzten 2 Jahre und Erbeben der Starke 2.5 und höher\n",
    "    - Daten sind nur von Stationen die mit dem United State Geological Service zusammenarbeiten, es kann nicht ausgeschlossen werden das Daten fehlen\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "\n",
    "#Einlesen der Daten, das USGS erlaubt nur Querrys mit max. 20000 Daten. Für die letzten 2 Jahre werden ca. 80000 Daten benötigt\n",
    "earthquake1=pd.read_csv(\"Erdbeben1.csv\")\n",
    "earthquake2=pd.read_csv(\"Erdbeben2.csv\")\n",
    "earthquake3=pd.read_csv(\"Erdbeben3.csv\")\n",
    "earthquake4=pd.read_csv(\"Erdbeben4.csv\")\n",
    "earthquake5=pd.read_csv(\"Erdbeben5.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ausgeben der Spaltennamen\n",
    "earthquake1.head()\n",
    "#Vor dem Einlesen der csv wurde überprüft, ob die Tabellen von den Spalten aus identisch sind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Überprüfen ob alle Spaltennamen ausgegeben wurden\n",
    "for col in earthquake1.columns:\n",
    "    print(col)\n",
    "\n",
    "# Information about this topics: https://earthquake.usgs.gov/data/comcat/data-eventterms.php\n",
    "\n",
    "#time: Wann ist das Erdbeben passiert\n",
    "#latidude: Geographische Koordinate\n",
    "#longitude: Geographische Koordinate\n",
    "#depth: Tiefe des Erdbeben\n",
    "#mag: Stärke auf Richterskala\n",
    "#magType: Magnitude Types (More Infos: https://www.usgs.gov/natural-hazards/earthquake-hazards/science/magnitude-types?qt-science_center_objects=0#qt-science_center_objects)\n",
    "#nst: Number of seismic stations which reported P- and S-arrival times for this earthquake\n",
    "#gap: azimuthal gap\n",
    "#dmin: Horizontal distance from the epicenter to the nearest station\n",
    "#rms: root-mean-squared residual of solution\n",
    "#net: The ID of a data contributor\n",
    "#id: A unique identifier for the event\n",
    "#updated: time when the event was most recently updated\n",
    "#place: Textual description of named geographic region near to the event\n",
    "#type: Type of seismic event\n",
    "#horizontalError: he horizontal location error, in km, defined as the length of the largest projection of the three principal errors on a horizontal plane\n",
    "#depthError: The depth error, in km, defined as the largest projection of the three principal errors on a vertical line\n",
    "#magError: Uncertainty of reported magnitude of the event\n",
    "#magNST: The total number of seismic stations used to calculate the magnitude for this earthquake\n",
    "#status: Status is either automatic or reviewed\n",
    "#locationSource: Decimal degrees longitude. Negative values for western longitudes\n",
    "#magSource: The method or algorithm used to calculate the preferred magnitude for the event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Genutzt um Tabelle anzuzeigen, für das eigentliche Programm nicht zwingend nötig\n",
    "#earthquake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aufteilen des Inhalts der Place Spalte für eine Leichtere Analyse im Späteren Verlauf, muss auf jedes Ursprüngliche Dataframe ausgeführt werden \n",
    "# Neue Spalte earthquake1\n",
    "earthquake1['Region']=earthquake1['place']\n",
    "\n",
    "#Länge der Tabelle\n",
    "size=earthquake1['Region'].size\n",
    "#Füllen der neuen Spalte\n",
    "for x in range(size):\n",
    "    #Leere Spalten überspringen\n",
    "    if isinstance(earthquake1['Region'][x], float):\n",
    "        continue\n",
    "    #Aufteilen des Inhalts in Region anhand der Kommas    \n",
    "    Region = earthquake1['Region'][x].split(',', 2)\n",
    "    #Es wird der letzte Teil des Splits benötigt der entwerder 3 oder 2 Lang ist\n",
    "    #Länge 2\n",
    "    if len(Region)==1:\n",
    "        earthquake1['Region'][x]=Region[0]\n",
    "    #Länge 3    \n",
    "    if len(Region)==2:\n",
    "        earthquake1['Region'][x]=Region[1] \n",
    "        \n",
    "#Aufteilen der time für leichtere Analyse muss vor concat gemacht werden\n",
    "\n",
    "# Erstellen neuer Spalten gleicher Größe\n",
    "earthquake1['Jahr']=earthquake1['place']\n",
    "earthquake1['Monat']=earthquake1['place']\n",
    "earthquake1['Tag']=earthquake1['place']\n",
    "earthquake1['Total']=earthquake1['place']\n",
    "\n",
    "#Aufteilen der Zeit\n",
    "size=earthquake1['Region'].size\n",
    "for x in range(size):\n",
    "    time=earthquake1['time'][x]\n",
    "    Jahr=time[0:4]\n",
    "    earthquake1['Jahr'][x]=Jahr    \n",
    "    Monat=time[5:7]\n",
    "    earthquake1['Monat'][x]=Monat\n",
    "    Tag=time[9:10]\n",
    "    earthquake1['Tag'][x]=Tag\n",
    "    Total=time[0:10]\n",
    "    earthquake1['Total'][x]=Total\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neue Spalte earthquake2\n",
    "earthquake2['Region']=earthquake2['place']\n",
    "\n",
    "#Länge der Tabell\n",
    "size=earthquake2['Region'].size\n",
    "#Füllen der neuen Spalte\n",
    "for x in range(size):\n",
    "    #Leere Spalten überspringen\n",
    "    if isinstance(earthquake2['Region'][x], float):\n",
    "        continue\n",
    "    #Aufteilen des Inhalts in Region anhand der Kommas    \n",
    "    Region = earthquake2['Region'][x].split(',', 2)\n",
    "    #Es wird der letzte Teil des Splits benötigt der entwerder 3 oder 2 Lang ist\n",
    "    #Länge 2\n",
    "    if len(Region)==1:\n",
    "        earthquake2['Region'][x]=Region[0]\n",
    "    #Länge 3    \n",
    "    if len(Region)==2:\n",
    "        earthquake2['Region'][x]=Region[1]\n",
    "        \n",
    "        \n",
    "#Aufteilen der time für leichtere Analyse muss vor concat gemacht werden\n",
    "earthquake2['Jahr']=earthquake2['place']\n",
    "earthquake2['Monat']=earthquake2['place']\n",
    "earthquake2['Tag']=earthquake2['place']\n",
    "earthquake2['Total']=earthquake2['place']\n",
    "\n",
    "#Aufteilen der Zeit\n",
    "size=earthquake2['Region'].size\n",
    "for x in range(size):\n",
    "    time=earthquake2['time'][x]\n",
    "    Jahr=time[0:4]\n",
    "    earthquake2['Jahr'][x]=Jahr    \n",
    "    Monat=time[5:7]\n",
    "    earthquake2['Monat'][x]=Monat\n",
    "    Tag=time[9:10]\n",
    "    earthquake2['Tag'][x]=Tag\n",
    "    Total=time[0:10]\n",
    "    earthquake2['Total'][x]=Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neue Spalte earthquake3\n",
    "earthquake3['Region']=earthquake3['place']\n",
    "\n",
    "#Länge der Tabelle\n",
    "size=earthquake3['Region'].size\n",
    "#Füllen der neuen Spalte\n",
    "for x in range(size):\n",
    "    #Leere Spalten überspringen\n",
    "    if isinstance(earthquake3['Region'][x], float):\n",
    "        continue\n",
    "    #Aufteilen des Inhalts in Region anhand der Kommas    \n",
    "    Region = earthquake3['Region'][x].split(',', 2)\n",
    "    #Es wird der letzte Teil des Splits benötigt der entwerder 3 oder 2 Lang ist\n",
    "    #Länge 2\n",
    "    if len(Region)==1:\n",
    "        earthquake3['Region'][x]=Region[0]\n",
    "    #Länge 3    \n",
    "    if len(Region)==2:\n",
    "        earthquake3['Region'][x]=Region[1]\n",
    "       \n",
    "    \n",
    "#Aufteilen der time für leichtere Analyse muss vor concat gemacht werden\n",
    "earthquake3['Jahr']=earthquake3['place']\n",
    "earthquake3['Monat']=earthquake3['place']\n",
    "earthquake3['Tag']=earthquake3['place']\n",
    "earthquake3['Total']=earthquake3['place']\n",
    "\n",
    "#Aufteilen der Zeit\n",
    "size=earthquake3['Region'].size\n",
    "for x in range(size):\n",
    "    time=earthquake3['time'][x]\n",
    "    Jahr=time[0:4]\n",
    "    earthquake3['Jahr'][x]=Jahr    \n",
    "    Monat=time[5:7]\n",
    "    earthquake3['Monat'][x]=Monat\n",
    "    Tag=time[9:10]\n",
    "    earthquake3['Tag'][x]=Tag\n",
    "    Total=time[0:10]\n",
    "    earthquake3['Total'][x]=Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neue Spalte earthquake4\n",
    "earthquake4['Region']=earthquake4['place']\n",
    "\n",
    "#Länge der Tabelle\n",
    "size=earthquake4['Region'].size\n",
    "#Füllen der neuen Spalte\n",
    "for x in range(size):\n",
    "    #Leere Spalten überspringen\n",
    "    if isinstance(earthquake4['Region'][x], float):\n",
    "        continue\n",
    "    #Aufteilen des Inhalts in Region anhand der Kommas    \n",
    "    Region = earthquake4['Region'][x].split(',', 2)\n",
    "    #Es wird der letzte Teil des Splits benötigt der entwerder 3 oder 2 Lang ist\n",
    "    #Länge 2\n",
    "    if len(Region)==1:\n",
    "        earthquake4['Region'][x]=Region[0]\n",
    "    #Länge 3    \n",
    "    if len(Region)==2:\n",
    "        earthquake4['Region'][x]=Region[1]\n",
    "        \n",
    "\n",
    "#Aufteilen der time für leichtere Analyse muss vor concat gemacht werden\n",
    "earthquake4['Jahr']=earthquake4['place']\n",
    "earthquake4['Monat']=earthquake4['place']\n",
    "earthquake4['Tag']=earthquake4['place']\n",
    "earthquake4['Total']=earthquake4['place']\n",
    "\n",
    "#Aufteilen der Zeit\n",
    "size=earthquake4['Region'].size\n",
    "for x in range(size):\n",
    "    time=earthquake4['time'][x]\n",
    "    Jahr=time[0:4]\n",
    "    earthquake4['Jahr'][x]=Jahr    \n",
    "    Monat=time[5:7]\n",
    "    earthquake4['Monat'][x]=Monat\n",
    "    Tag=time[9:10]\n",
    "    earthquake4['Tag'][x]=Tag\n",
    "    Total=time[0:10]\n",
    "    earthquake4['Total'][x]=Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neue Spalte earthquake5\n",
    "earthquake5['Region']=earthquake5['place']\n",
    "\n",
    "#Länge der Tabelle\n",
    "size=earthquake5['Region'].size\n",
    "#Füllen der neuen Spalte\n",
    "for x in range(size):\n",
    "    #Leere Spalten überspringen\n",
    "    if isinstance(earthquake5['Region'][x], float):\n",
    "        continue\n",
    "    #Aufteilen des Inhalts in Region anhand der Kommas    \n",
    "    Region = earthquake5['Region'][x].split(',', 2)\n",
    "    #Es wird der letzte Teil des Splits benötigt der entwerder 3 oder 2 Lang ist\n",
    "    #Länge 2\n",
    "    if len(Region)==1:\n",
    "        earthquake5['Region'][x]=Region[0]\n",
    "    #Länge 3    \n",
    "    if len(Region)==2:\n",
    "        earthquake5['Region'][x]=Region[1]\n",
    "\n",
    "        \n",
    "#Aufteilen der time für leichtere Analyse muss vor concat gemacht werden\n",
    "earthquake5['Jahr']=earthquake5['place']\n",
    "earthquake5['Monat']=earthquake5['place']\n",
    "earthquake5['Tag']=earthquake5['place']\n",
    "earthquake5['Total']=earthquake5['place']\n",
    "\n",
    "#Aufteilen der Zeit\n",
    "size=earthquake5['Region'].size\n",
    "for x in range(size):\n",
    "    time=earthquake5['time'][x]\n",
    "    Jahr=time[0:4]\n",
    "    earthquake5['Jahr'][x]=Jahr    \n",
    "    Monat=time[5:7]\n",
    "    earthquake5['Monat'][x]=Monat\n",
    "    Tag=time[9:10]\n",
    "    earthquake5['Tag'][x]=Tag\n",
    "    Total=time[0:10]\n",
    "    earthquake5['Total'][x]=Total\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Zusammenführen der Daten in ein einziges Dataframe\n",
    "earthquake = pd.concat([earthquake1, earthquake2, earthquake3, earthquake4, earthquake5])\n",
    "\n",
    "# Backup Dataframe\n",
    "earthquakeBackup = pd.concat([earthquake1, earthquake2, earthquake3, earthquake4, earthquake5])\n",
    "#für Chi-Quadrat nötig\n",
    "#earthquakeChi = pd.concat([earthquake1, earthquake2, earthquake3, earthquake4, earthquake5])\n",
    "# Performing cleaning tasks here\n",
    "#total numbers of columns and rows\n",
    "print(earthquake.shape)\n",
    "#insgesamt 80827 Zeilen und 22 Spalten\n",
    "\n",
    "# Grundsätzliche Informationen über das Dataframe\n",
    "print(earthquake.info())\n",
    "# alle Spalten mit weniger als 80827 Werten haben lücken\n",
    "# magtype: für Analyse interresant und muss beachtet werden\n",
    "#nst: Hier unwichtig\n",
    "#gap: Hier unwichtig\n",
    "#dmin: Hier unwichtig\n",
    "#rms: Hier unwichtig\n",
    "#Place: Muss beachtet werden\n",
    "#...Error: Hier unwichtig, für eine Analyse der Güte der Daten aber villeicht intterresant\n",
    "#magNST: hier unwichtig\n",
    "\n",
    "# Datentypen Object und float64, im Moment Ok, müssen in Späterem Verlauf vielleicht beachtet werden\n",
    "\n",
    "#Löschen der Unwichtigen/unvollständiger Daten \n",
    "earthquakeChanged=earthquake.drop(['nst', 'gap', 'dmin', 'rms', 'latitude', 'longitude', 'net', 'updated', 'horizontalError', 'depthError', 'magError', 'magNst', 'status', 'locationSource', 'magSource'], axis=1)\n",
    "\n",
    "#Informationen über das neue Dataframe\n",
    "print(earthquakeChanged.info())\n",
    "#Fehlende Daten für magType und place müssen beachtet werden\n",
    "\n",
    "# check if data missing\n",
    "missing = earthquakeChanged.isnull().sum()\n",
    "print(missing) \n",
    "#Fehlende Daten sind bereits bekannt un werden bei der Analyse beachtet\n",
    "\n",
    "#Alle Datentypen umwandeln\n",
    "earthquakeChanged = earthquakeChanged.astype({\"Region\": str, 'type': str})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing EDA \n",
    "# Aufgabe 1.1 a\n",
    "#Print different Bee Swarm strength to depth/Region/type/Jahr/Monat nicht möglich, Rechenzeit zu lang\n",
    "#Es wird Versuch über andere Diagramm Arten die Daten zu visualisieren\n",
    "#max = sns.swarmplot(x=\"Jahr\", y=\"mag\", data=earthquakeChanged)\n",
    "\n",
    "# Erstelle den Plot für mag\n",
    "# Erstelle den Plot\n",
    "_ = plt.hist(earthquakeChanged['mag'])\n",
    "\n",
    "\n",
    "plt.title('Magnitude')\n",
    "plt.xlabel('Magnitude')\n",
    "plt.ylabel('Number of Earthquakes')\n",
    "\n",
    "#Graphik zeigt die Verteilun der Erbebenstärke in BEzug auf die Menge der Erdbeben\n",
    "\n",
    "# Zeige Plot an\n",
    "plt.show()\n",
    "\n",
    "# Kleine Erdbeben sind deutlich häufiger\n",
    "\n",
    "# Erstelle den Plot\n",
    "_ = plt.hist(earthquakeChanged['Jahr'])\n",
    "\n",
    "\n",
    "plt.title('Jahres Vergleich')\n",
    "plt.xlabel('Jahr')\n",
    "plt.ylabel('Number of Earthquakes')\n",
    "\n",
    "# Zeigt die Menge an Erdbeben pro Jahr\n",
    "\n",
    "# Zeige Plot an\n",
    "plt.show()\n",
    "\n",
    "#Erdbeben im Jahr 2020 häufiger\n",
    "\n",
    "# Erstelle den Plot\n",
    "_ = plt.hist(earthquakeChanged['Monat'])\n",
    "\n",
    "\n",
    "plt.title('Monats Vergleich')\n",
    "plt.xlabel('Monat')\n",
    "plt.ylabel('Number of Earthquakes')\n",
    "\n",
    "# Zeigt die Menge an Erdbeben pro Monat\n",
    "\n",
    "#Erdbeben an Ende des Jahres deutlich häufiger\n",
    "\n",
    "# Zeige Plot an\n",
    "plt.show()\n",
    "\n",
    "# Erstelle den Plot\n",
    "_ = plt.hist(earthquakeChanged['depth'])\n",
    "\n",
    "\n",
    "plt.title('Tiefe')\n",
    "plt.xlabel('Tiefe')\n",
    "plt.ylabel('Number of Earthquakes')\n",
    "\n",
    "#Anzahl der Erdbeben in BEzug auf die Tiefe\n",
    "\n",
    "# Zeige Plot an\n",
    "plt.show()\n",
    "\n",
    "#Erdbeben selten in großer Tiefe\n",
    "\n",
    "# Zeige Plot an\n",
    "plt.show()\n",
    "\n",
    "# Erstelle den Plot\n",
    "_ = plt.hist(earthquakeChanged['type'])\n",
    "\n",
    "\n",
    "plt.title('Type')\n",
    "plt.xlabel('Type')\n",
    "plt.ylabel('Number of Earthquakes')\n",
    "\n",
    "#Wie häufig ist welcher Erdbeben-Typ\n",
    "\n",
    "# Zeige Plot an\n",
    "plt.show()\n",
    "# Ein Type ist deutlich häufiger als die anderen, im Histogramm aber schlecht sichtbar\n",
    "\n",
    "# Erstelle den Plot\n",
    "#_ = plt.hist(earthquakeChanged['Region'])\n",
    "#plt.title('Region')\n",
    "#plt.xlabel('Region')\n",
    "#plt.ylabel('Number of Earthquakes')\n",
    "# Zeige Plot an\n",
    "#plt.show()\n",
    "#Nicht sinnvoll, da es zu viele verschiedene Locations gibt. Die möglichkeit des binnings von Data ist hier nicht sinnvoll, da es keine sinnvolle Möglichkeit gibt die Werte zu gruppieren ohne weitere Daten einzulesen\n",
    "\n",
    "#Heatmap für Correlation\n",
    "#Werte Numerisch machen\n",
    "earthquakeChanged = earthquakeChanged.astype({\"Monat\": float, 'Jahr': float})\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(earthquakeChanged.corr(),cbar=True,annot=True,cmap='Blues')\n",
    "plt.title('Heatmap Correlation Numerical Data')\n",
    "plt.show()\n",
    "#Zeigt die Korrelation zwischen den numerischen Werten\n",
    "\n",
    "#Erste Analyse zeigt, das Werte Jahr, Monat, Tiefe und Stärke nicht oder nur geringfügig Korrelieren\n",
    "\n",
    "#distinct location Sources (Nur bedingt aussagekräftig)\n",
    "#distinctLocationSource = []\n",
    "#for x in earthquakeChanged.Region:\n",
    "#    if x not in distinctLocationSource:\n",
    "#        distinctLocationSource.append(x)\n",
    "#print(distinctLocationSource)\n",
    "\n",
    "# An einigen Orten sind Erdbeben deutlich häufiger (Alaska, Puerto Rico, Ca, Indonesia, Philipines)\n",
    "\n",
    "#Berrechnung median mag and depth\n",
    "median_mag = statistics.median(earthquakeChanged['mag'])\n",
    "print('median value of mag:', median_mag)\n",
    "\n",
    "median_depth = statistics.median(earthquakeChanged['depth'])\n",
    "print('median value of depth in km:', median_depth)\n",
    "\n",
    "#Berrechnung mean mag and depth\n",
    "mean_mag = np.mean(earthquakeChanged['mag'])\n",
    "print('mean value of mag:', median_mag)\n",
    "\n",
    "mean_depth = np.mean(earthquakeChanged['depth'])\n",
    "print('mean value of depth in km:', median_depth)\n",
    "\n",
    "#mean und median sind exakt gleich\n",
    "\n",
    "#Berrechnung Percentile\n",
    "np.sort(earthquakeChanged['depth'])\n",
    "percentiles = np.array([2.5, 25, 50, 75, 97.5])\n",
    "ptiles_depth = np.percentile(earthquakeChanged['depth'], percentiles)\n",
    "print('percentiles of depth 2.5, 25, 50, 75, 97.5',ptiles_depth)\n",
    "\n",
    "np.sort(earthquakeChanged['mag'])\n",
    "percentiles = np.array([2.5, 25, 50, 75, 97.5])\n",
    "ptiles_mag = np.percentile(earthquakeChanged['mag'], percentiles)\n",
    "print('percentiles of mag 2.5, 25, 50, 75, 97.5',ptiles_mag)\n",
    "\n",
    "# Variance and Standard Deviation\n",
    "#Bringen in diesem Fall nur wenig\n",
    "\n",
    "# Variance and Standard Deviation depth\n",
    "print('Varianz und Std Tiefe')\n",
    "# Array of differences to mean: differences\n",
    "differences = earthquakeChanged['depth'] - np.mean(earthquakeChanged['depth'])\n",
    "\n",
    "# Square the differences: diff_sq\n",
    "diff_sq = differences**2\n",
    "\n",
    "# Compute the mean square difference: variance by its own\n",
    "variance_explicit = np.mean(diff_sq)\n",
    "\n",
    "# Compute the variance using NumPy: variance_np\n",
    "variance_np = np.var(earthquakeChanged['depth'])\n",
    "\n",
    "# Print the results\n",
    "print(variance_explicit, variance_np)\n",
    "\n",
    "#Standard Deviation is the square root of the variance\n",
    "\n",
    "variance = np.var(earthquakeChanged['depth'])\n",
    "\n",
    "# Print the square root of the variance\n",
    "print(np.sqrt(variance))\n",
    "\n",
    "# Variance and Standard Deviation mag\n",
    "\n",
    "print('Varianz und Std Mag')\n",
    "# Array of differences to mean: differences\n",
    "differences = earthquakeChanged['mag'] - np.mean(earthquakeChanged['mag'])\n",
    "\n",
    "# Square the differences: diff_sq\n",
    "diff_sq = differences**2\n",
    "\n",
    "# Compute the mean square difference: variance by its own\n",
    "variance_explicit = np.mean(diff_sq)\n",
    "\n",
    "# Compute the variance using NumPy: variance_np\n",
    "variance_np = np.var(earthquakeChanged['mag'])\n",
    "\n",
    "# Print the results\n",
    "print(variance_explicit, variance_np)\n",
    "\n",
    "#Standard Deviation is the square root of the variance\n",
    "\n",
    "variance = np.var(earthquakeChanged['mag'])\n",
    "\n",
    "# Print the square root of the variance\n",
    "print(np.sqrt(variance))\n",
    "\n",
    "#Durchschnittliche Tiefe der Beben nach Region\n",
    "print(earthquakeChanged.groupby(['Region'])['depth'].agg(lambda x: x.unique().mean()).sort_values(ascending=False))\n",
    "#Durchschnittliche Stärke der Beben nach Region\n",
    "print(earthquakeChanged.groupby(['Region'])['mag'].agg(lambda x: x.unique().mean()).sort_values(ascending=False))\n",
    "# Es gibt sehr viele Unterschiedliche Orte, an denen ein Erdbeben auftritt) \n",
    "print(earthquakeChanged['Region'].value_counts()) \n",
    "\n",
    "#Boxplot depth\n",
    "plt.title('Standard Statistics depth')\n",
    "plt.ylabel('Tiefe in km')\n",
    "plt.rcParams[\"figure.figsize\"] = [7.50, 3.50]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "data = earthquakeChanged['depth']\n",
    "min = data.min(0)\n",
    "max = data.max(0)\n",
    "avg = data.mean(0)\n",
    "std = data.std(0)\n",
    "df = pd.DataFrame(dict(min=min, max=max, avg=avg, std=std), index=[0])\n",
    "df.boxplot()\n",
    "plt.show()\n",
    "\n",
    "#zeigt Grundsätzliche Statistiken für die Tiefe\n",
    "\n",
    "#Boxplot mag\n",
    "plt.title('Standard Statistics mag')\n",
    "plt.ylabel('Magnitude')\n",
    "plt.rcParams[\"figure.figsize\"] = [7.50, 3.50]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "data = earthquakeChanged['mag']\n",
    "min = data.min(0)\n",
    "max = data.max(0)\n",
    "avg = data.mean(0)\n",
    "std = data.std(0)\n",
    "df = pd.DataFrame(dict(min=min, max=max, avg=avg, std=std), index=[0])\n",
    "df.boxplot()\n",
    "plt.show()\n",
    "#zeigt Grundsätzliche Statistiken für die Magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aufgabe 1.1 b\n",
    "def pearson_r(x, y):\n",
    "    \"\"\"Compute Pearson correlation coefficient between two arrays.\"\"\"\n",
    "    # Compute correlation matrix: corr_mat\n",
    "    corr_mat = np.corrcoef(x, y)\n",
    "\n",
    "    # Return entry [0,1]\n",
    "    return corr_mat[0,1]\n",
    "\n",
    "# Compute Pearson correlation coefficient for Depth and mag\n",
    "print('Pearson Correlation')\n",
    "r = pearson_r(earthquakeChanged['mag'], earthquakeChanged['depth'])\n",
    "\n",
    "# Print the result\n",
    "print(r)\n",
    "print('Maß für den Grad des linearen Zusammenhangs. Pearson Correlation beider Variablen schwach positiv, wenn ein Wert sich erhöht dann erhäht sich der andere ebenfalls (selbes geht für decrease)')\n",
    "print('It gives information about the magnitude of the association, or correlation, as well as the direction of the relationship, Depth und Mag Korrelieren nur schwach')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Brauche ich später\n",
    "\n",
    "#Durchschnittliche Tiefe der Beben nach Region unsortiert\n",
    "print(earthquakeChanged.groupby(['Region'])['depth'].agg(lambda x: x.unique().mean()))\n",
    "#Durchschnittliche Stärke der Beben nach Region unsortiert\n",
    "print(earthquakeChanged.groupby(['Region'])['mag'].agg(lambda x: x.unique().mean()))\n",
    "# Es gibt sehr viele Unterschiedliche Orte, an denen ein Erdbeben auftritt)\n",
    "# Stellt Min und Maxwerte dar für Stärke auf jede Region bezogen\n",
    "print(earthquakeChanged.groupby(['Region'])['mag'].agg(lambda x: x.unique().min()))\n",
    "print(earthquakeChanged.groupby(['Region'])['mag'].agg(lambda x: x.unique().max()))\n",
    "# Stellt Min und Maxwerte dar für Tiefe auf jede Region bezogen\n",
    "print(earthquakeChanged.groupby(['Region'])['depth'].agg(lambda x: x.unique().min()))\n",
    "print(earthquakeChanged.groupby(['Region'])['depth'].agg(lambda x: x.unique().max()))\n",
    "\n",
    "#nur schlecht visualisierbar, aufgrund der vielzahl von Daten wird das ganze nicht Übersichtlich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aufgabe 1.2 a/b\n",
    "#mag/depth scatterplot\n",
    "y = earthquakeChanged['mag']\n",
    "x = earthquakeChanged['depth']\n",
    "\n",
    "plt.figure(figsize=(17,10))\n",
    "plt.title('mag/depth')\n",
    "plt.xlabel('depth in km')\n",
    "plt.ylabel('mag')\n",
    "plt.plot(x, y, '.');\n",
    "plt.show()\n",
    "\n",
    "#depth/mag scatterplot\n",
    "x = earthquakeChanged['mag']\n",
    "y = earthquakeChanged['depth']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(17,10))\n",
    "plt.title('mag/depth')\n",
    "plt.xlabel('mag')\n",
    "plt.ylabel('depth in km')\n",
    "plt.plot(x, y, '.');\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Polyfit für mean Diagramme\n",
    "\n",
    "#---LINEAR FIT----\n",
    "#lineare Regression mag/depth\n",
    "#generate the x array\n",
    "x = earthquakeChanged['depth']\n",
    "\n",
    "#generate the y array exploiting the random.randint() function to introduce some random noise\n",
    "y = earthquakeChanged['mag']\n",
    "#Applying a linear fit with .polyfit()\n",
    "fit = np.polyfit(x,y,1)\n",
    "ang_coeff = fit[0]\n",
    "intercept = fit[1]\n",
    "fit_eq = ang_coeff*x + intercept  #obtaining the y axis values for the fitting function\n",
    "\n",
    "##lineare Regression depth/mag Gleichung ausgeben\n",
    "mdf=np.poly1d(fit)\n",
    "print('lineare Regression depth/mag' + str(mdf))\n",
    "#Plotting the data\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.subplots()\n",
    "ax.plot(x, fit_eq,color = 'r', alpha = 0.5, label = 'Linear fit')\n",
    "ax.scatter(x,y,s = 5, color = 'b', label = 'Data points') #Original data points\n",
    "ax.set_title('Linear fit depth/mag')\n",
    "plt.ylabel('depth in km')\n",
    "plt.xlabel('mag')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#---LINEAR FIT----\n",
    "#lineare Regression depth/mag\n",
    "#generate the x array\n",
    "x = earthquakeChanged['mag']\n",
    "\n",
    "#generate the y array exploiting the random.randint() function to introduce some random noise\n",
    "y = earthquakeChanged['depth']\n",
    "#Applying a linear fit with .polyfit()\n",
    "fit = np.polyfit(x,y,1)\n",
    "ang_coeff = fit[0]\n",
    "intercept = fit[1]\n",
    "fit_eq = ang_coeff*x + intercept  #obtaining the y axis values for the fitting function\n",
    "##lineare Regression depth/mag Gleichung ausgeben\n",
    "dmf=np.poly1d(fit)\n",
    "print('lineare Regression mag/depth' + str(dmf))\n",
    "#Plotting the data\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.subplots()\n",
    "ax.plot(x, fit_eq,color = 'r', alpha = 0.5, label = 'Linear fit')\n",
    "ax.scatter(x,y,s = 5, color = 'b', label = 'Data points') #Original data points\n",
    "ax.set_title('Linear fit mag/depth')\n",
    "plt.xlabel('depth in km')\n",
    "plt.ylabel('mag')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "print('Unit of the Linear Fit for depth/mag: km')\n",
    "\n",
    "print('Unit of the Linear Fit for mag/depth: 1/km')\n",
    "\n",
    "print('Magnitude oder die Stärke von Erdbeben ist auf der Richterskala ohne Einheit')\n",
    "\n",
    "#Mean mag/depth Diagramm\n",
    "\n",
    "y = earthquakeChanged.groupby(['Region'])['depth'].agg(lambda x: x.unique().mean())\n",
    "x = earthquakeChanged.groupby(['Region'])['mag'].agg(lambda x: x.unique().mean())\n",
    "\n",
    "plt.figure(figsize=(17,10))\n",
    "plt.title('mean mag/depth')\n",
    "plt.xlabel('depth in km')\n",
    "plt.ylabel('mag')\n",
    "plt.plot(x, y, '.')\n",
    "plt.show()\n",
    "#Mean depth/mag Diagramm\n",
    "\n",
    "x = earthquakeChanged.groupby(['Region'])['depth'].agg(lambda x: x.unique().mean())\n",
    "y = earthquakeChanged.groupby(['Region'])['mag'].agg(lambda x: x.unique().mean())\n",
    "\n",
    "plt.figure(figsize=(17,10))\n",
    "plt.title('mean mag/depth')\n",
    "plt.xlabel('depth in km')\n",
    "plt.ylabel('mag')\n",
    "plt.plot(x, y, '.')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#---LINEAR FIT----\n",
    "#lineare Regression mag/depth \n",
    "#generate the x array\n",
    "y = earthquakeChanged.groupby(['Region'])['depth'].agg(lambda x: x.unique().mean())\n",
    "\n",
    "#generate the y array exploiting the random.randint() function to introduce some random noise\n",
    "x = earthquakeChanged.groupby(['Region'])['mag'].agg(lambda x: x.unique().mean())\n",
    "#Applying a linear fit with .polyfit()\n",
    "fit = np.polyfit(x,y,1)\n",
    "ang_coeff = fit[0]\n",
    "intercept = fit[1]\n",
    "fit_eq = ang_coeff*x + intercept  #obtaining the y axis values for the fitting function\n",
    "##lineare Regression depth/mag Gleichung ausgeben\n",
    "f=np.poly1d(fit)\n",
    "print('lineare Regression depth/mag' + str(f))\n",
    "#Plotting the data\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.subplots()\n",
    "ax.plot(x, fit_eq,color = 'r', alpha = 0.5, label = 'Linear fit')\n",
    "ax.scatter(x,y,s = 5, color = 'b', label = 'Data points') #Original data points\n",
    "ax.set_title('Linear fit depth/mag')\n",
    "plt.ylabel('mag')\n",
    "plt.xlabel('depth in km')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#---LINEAR FIT----\n",
    "#lineare Regression depth/mag unsortiert\n",
    "#generate the x array\n",
    "y = earthquakeChanged.groupby(['Region'])['mag'].agg(lambda x: x.unique().mean())\n",
    "\n",
    "#generate the y array exploiting the random.randint() function to introduce some random noise\n",
    "x = earthquakeChanged.groupby(['Region'])['depth'].agg(lambda x: x.unique().mean())\n",
    "#Applying a linear fit with .polyfit()\n",
    "fit = np.polyfit(x,y,1)\n",
    "ang_coeff = fit[0]\n",
    "intercept = fit[1]\n",
    "fit_eq = ang_coeff*x + intercept  #obtaining the y axis values for the fitting function\n",
    "##lineare Regression depth/mag Gleichung ausgeben\n",
    "f=np.poly1d(fit)\n",
    "print('lineare Regression mag/depth' + str(f))\n",
    "#Plotting the data\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.subplots()\n",
    "ax.plot(x, fit_eq,color = 'r', alpha = 0.5, label = 'Linear fit')\n",
    "ax.scatter(x,y,s = 5, color = 'b', label = 'Data points') #Original data points\n",
    "ax.set_title('Linear fit depth/mag')\n",
    "plt.xlabel('mag')\n",
    "plt.ylabel('depth in km')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auch wenn die Sortierten Graphilen mit der linearen Regression die schönsten Werte liefer\n",
    "### sind die Verknüpfungen die hier gezogen Werten Falsch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chosen that model\n",
    "\n",
    "Bei dieser Aufgabe gibt es kein wirkliches Data Model, es wurden lediglich CSV Dateien eingelesen\n",
    "\n",
    "#### 3.2 Hypothesis Tests\n",
    "provide needed statistiks with explonation and answer to your business questions for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual Data Model\n",
    "# Aufgabe 1.3 a\n",
    "\n",
    "#lineare regression mean depth/mag\n",
    "#23.85x-55.41\n",
    "#a=23.85\n",
    "b=55.41\n",
    "\n",
    "earthquakeChanged = earthquakeChanged.astype({\"mag\": float, 'depth': float})\n",
    "\n",
    "N=200\n",
    "a_vals=np.linspace(0, 0.1, N, endpoint=True)\n",
    "rss=np.empty_like(a_vals)\n",
    "for i,a in enumerate(a_vals):\n",
    "    rss[i]=np.sum((earthquakeChanged.groupby(['Region'])['depth'].agg(lambda x: x.unique().mean())-a*earthquakeChanged.groupby(['Region'])['mag'].agg(lambda x: x.unique().mean())-b)**2)\n",
    "plt.title('Aufgabe 1.3')\n",
    "plt.plot(a_vals, rss, \"-\")\n",
    "plt.xlabel(\"slope [depth/mag]\")\n",
    "plt.ylabel('sum of square of residuals')\n",
    "plt.show()\n",
    "\n",
    "#lineare regression mean mag/depth\n",
    "\n",
    "#0.00304x + 3.892\n",
    "\n",
    "b=3.892\n",
    "earthquakeChanged = earthquakeChanged.astype({\"mag\": float, 'depth': float})\n",
    "\n",
    "N=200\n",
    "a_vals=np.linspace(0, 0.1, N, endpoint=True)\n",
    "rss=np.empty_like(a_vals)\n",
    "for i,a in enumerate(a_vals):\n",
    "    rss[i]=np.sum((earthquakeChanged.groupby(['Region'])['mag'].agg(lambda x: x.unique().mean())-a*earthquakeChanged.groupby(['Region'])['depth'].agg(lambda x: x.unique().mean())-b)**2)\n",
    "plt.title('Aufgabe 1.3')\n",
    "plt.plot(a_vals, rss, \"-\")\n",
    "plt.xlabel(\"slope [mag/depth]\")\n",
    "plt.ylabel('sum of square of residuals')\n",
    "plt.show()\n",
    "\n",
    "print('rss und lineare regression unterschieden sich, Daten sind nicht normalverteilt')\n",
    "\n",
    "# Aufgabe 1.4 a\n",
    "\n",
    "\n",
    "\n",
    "def bootstrap_replicate_1d(data, func):\n",
    "    \"\"\"Generate bootstrap replicate of 1D data.\"\"\"\n",
    "    bs_sample = np.random.choice(data, len(data))\n",
    "    return func(bs_sample)\n",
    "\n",
    "def draw_bs_reps(data, func, size=1):\n",
    "    \"\"\"Draw bootstrap replicates.\"\"\"\n",
    "\n",
    "    # Initialize array of replicates: bs_replicates\n",
    "    bs_replicates = np.empty(size)\n",
    "\n",
    "    # Generate replicates\n",
    "    for i in range(size):\n",
    "        bs_replicates[i] = bootstrap_replicate_1d(data, func)\n",
    "\n",
    "    return bs_replicates\n",
    "\n",
    "#For mag\n",
    "#Bootstrap replicates of the mean and the SEM ( standard error of the mean )\n",
    "\n",
    "# Take 10,000 bootstrap replicates of the mean: bs_replicates\n",
    "bs_replicates = draw_bs_reps(earthquakeChanged.groupby(['Region'])['mag'].agg(lambda x: x.unique().mean()), np.mean, size=10000)\n",
    "\n",
    "# Compute and print SEM\n",
    "sem = np.std(earthquakeChanged.groupby(['Region'])['mag'].agg(lambda x: x.unique().mean())) / np.sqrt(len(earthquakeChanged.groupby(['Region'])['mag'].agg(lambda x: x.unique().mean())))\n",
    "print('SEM')\n",
    "print(sem)\n",
    "\n",
    "# Compute and print standard deviation of bootstrap replicates\n",
    "bs_std = np.std(bs_replicates)\n",
    "print('bs_std')\n",
    "print(bs_std)\n",
    "\n",
    "# Make a histogram of the results\n",
    "_ = plt.hist(bs_replicates, bins=50, density=True)\n",
    "_ = plt.xlabel('mag')\n",
    "_ = plt.ylabel('PDF')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Draw bootstrap replicates of the mean no-hitter time (equal to tau): bs_replicates\n",
    "bs_replicates = draw_bs_reps(earthquakeChanged.groupby(['Region'])['mag'].agg(lambda x: x.unique().mean()), np.mean, size=10000)\n",
    "\n",
    "# Compute the 95% confidence interval: conf_int\n",
    "conf_int = np.percentile(bs_replicates, [2.5, 97.5])\n",
    "\n",
    "# Print the confidence interval\n",
    "print('95% confidence interval =', conf_int, 'mag')\n",
    "\n",
    "# Plot the histogram of the replicates\n",
    "_ = plt.hist(bs_replicates, bins=50)\n",
    "_ = plt.xlabel('mean mag')\n",
    "_ = plt.ylabel('PDF')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#For mag\n",
    "#Bootstrap replicates of the mean and the SEM ( standard error of the mean )\n",
    "\n",
    "# Take 10,000 bootstrap replicates of the mean: bs_replicates\n",
    "bs_replicates = draw_bs_reps(earthquakeChanged.groupby(['Region'])['depth'].agg(lambda x: x.unique().mean()), np.mean, size=10000)\n",
    "\n",
    "# Compute and print SEM\n",
    "sem = np.std(earthquakeChanged.groupby(['Region'])['depth'].agg(lambda x: x.unique().mean())) / np.sqrt(len(earthquakeChanged.groupby(['Region'])['mag'].agg(lambda x: x.unique().mean())))\n",
    "print('SEM')\n",
    "print(sem)\n",
    "\n",
    "# Compute and print standard deviation of bootstrap replicates\n",
    "bs_std = np.std(bs_replicates)\n",
    "print('bs_std')\n",
    "print(bs_std)\n",
    "\n",
    "# Make a histogram of the results\n",
    "_ = plt.hist(bs_replicates, bins=50, density=True)\n",
    "_ = plt.xlabel('depth')\n",
    "_ = plt.ylabel('PDF')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Draw bootstrap replicates of the mean no-hitter time (equal to tau): bs_replicates\n",
    "bs_replicates = draw_bs_reps(earthquakeChanged.groupby(['Region'])['depth'].agg(lambda x: x.unique().mean()), np.mean, size=10000)\n",
    "\n",
    "# Compute the 95% confidence interval: conf_int\n",
    "conf_int = np.percentile(bs_replicates, [2.5, 97.5])\n",
    "\n",
    "# Print the confidence interval\n",
    "print('95% confidence interval =', conf_int, 'depth')\n",
    "\n",
    "# Plot the histogram of the replicates\n",
    "_ = plt.hist(bs_replicates, bins=50)\n",
    "_ = plt.xlabel('mean depth')\n",
    "_ = plt.ylabel('PDF')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aufgabe 1.5 a\n",
    "\n",
    "\n",
    "def draw_bs_pairs_linreg(x, y, size=1):\n",
    "    \"\"\"Perform pairs bootstrap for linear regression.\"\"\"\n",
    "\n",
    "    # Set up array of indices to sample from: inds\n",
    "    inds = np.arange(len(x))\n",
    "\n",
    "    # Initialize replicates: bs_slope_reps, bs_intercept_reps\n",
    "    bs_slope_reps = np.empty(size)\n",
    "    bs_intercept_reps = np.empty(size)\n",
    "\n",
    "    # Generate replicates\n",
    "    for i in range(size):\n",
    "        bs_inds = np.random.choice(inds, size=len(inds))\n",
    "        bs_x, bs_y = x[bs_inds], y[bs_inds]\n",
    "        bs_slope_reps[i], bs_intercept_reps[i] = np.polyfit(bs_x, bs_y, 1)\n",
    "\n",
    "    return bs_slope_reps, bs_intercept_reps\n",
    "\n",
    "# Generate replicates of slope and intercept using pairs bootstrap for depth/mag\n",
    "bs_slope_reps, bs_intercept_reps = draw_bs_pairs_linreg(\n",
    "                     earthquakeChanged.groupby(['Region'])['mag'].agg(lambda x: x.unique().mean()),  earthquakeChanged.groupby(['Region'])['depth'].agg(lambda x: x.unique().mean()), size=1000)\n",
    "\n",
    "# Compute and print 95% CI for slope\n",
    "print('95% CI for slope')\n",
    "print(np.percentile(bs_slope_reps, [2.5, 97.5]))\n",
    "\n",
    "# Plot the histogram\n",
    "_ = plt.hist(bs_slope_reps, bins=50)\n",
    "_ = plt.xlabel('slope')\n",
    "_ = plt.ylabel('PDF')\n",
    "plt.show()\n",
    "\n",
    "# Generate array of x-values for bootstrap lines: x\n",
    "x = np.array([0, 10])\n",
    "\n",
    "# Plot the bootstrap lines\n",
    "for i in range(100):\n",
    "    _ = plt.plot(x, \n",
    "                 bs_slope_reps[i] * x + bs_intercept_reps[i],\n",
    "                 linewidth=0.5, alpha=0.2, color='red')\n",
    "\n",
    "# Plot the data\n",
    "_ = plt.plot( earthquakeChanged.groupby(['Region'])['mag'].agg(lambda x: x.unique().mean()),  earthquakeChanged.groupby(['Region'])['depth'].agg(lambda x: x.unique().mean()), marker='.', linestyle='none')\n",
    "\n",
    "# Label axes, set the margins, and show the plot\n",
    "_ = plt.xlabel('mag')\n",
    "_ = plt.ylabel('depth')\n",
    "plt.title('depth/mag')\n",
    "plt.margins(0.02)\n",
    "plt.show()\n",
    "\n",
    "# Generate replicates of slope and intercept using pairs bootstrap for mag/depth\n",
    "bs_slope_reps, bs_intercept_reps = draw_bs_pairs_linreg(\n",
    "                     earthquakeChanged.groupby(['Region'])['depth'].agg(lambda x: x.unique().mean()),  earthquakeChanged.groupby(['Region'])['mag'].agg(lambda x: x.unique().mean()), size=1000)\n",
    "\n",
    "# Compute and print 95% CI for slope\n",
    "print('95% CI for slope')\n",
    "print(np.percentile(bs_slope_reps, [2.5, 97.5]))\n",
    "\n",
    "# Plot the histogram\n",
    "_ = plt.hist(bs_slope_reps, bins=50)\n",
    "_ = plt.xlabel('slope')\n",
    "_ = plt.ylabel('PDF')\n",
    "plt.show()\n",
    "\n",
    "# Generate array of x-values for bootstrap lines: x\n",
    "x = np.array([0, 700])\n",
    "\n",
    "# Plot the bootstrap lines\n",
    "for i in range(100):\n",
    "    _ = plt.plot(x, \n",
    "                 bs_slope_reps[i] * x + bs_intercept_reps[i],\n",
    "                 linewidth=0.5, alpha=0.2, color='red')\n",
    "\n",
    "# Plot the data\n",
    "_ = plt.plot( earthquakeChanged.groupby(['Region'])['depth'].agg(lambda x: x.unique().mean()),  earthquakeChanged.groupby(['Region'])['mag'].agg(lambda x: x.unique().mean()), marker='.', linestyle='none')\n",
    "\n",
    "# Label axes, set the margins, and show the plot\n",
    "_ = plt.xlabel('depth')\n",
    "_ = plt.ylabel('mag')\n",
    "plt.title('mag/depth')\n",
    "plt.margins(0.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis Test\n",
    "# Aufgabe 1.6 a\n",
    "# Hypothese 1:Tiefe und Stärke hängen voneinander ab\n",
    "# Gegenhypothese: Tiefe und Stärke hängen nicht voneinander ab\n",
    "print('Pearson Correlation')\n",
    "r = pearson_r(earthquakeChanged['mag'], earthquakeChanged['depth'])\n",
    "# Print the result\n",
    "print(r)\n",
    "print('Positive Pearson Correlation deutet darauf hin, das beide Werte voneinander abhängen. Weitere Tests wie z.B. Chi Quadrat machen an dieser Stelle keinen sinn, da die Daten nicht nominal sinnd, sich also nicht sinnvoll und zur Hypothese passen in Gruppen einteilen lassen.')\n",
    "print('Durch den Pearson Coeefizient wird die Gegenhypothese abgelehnt und es wird angenommen, das Tiefe eines Beben und Stärke voneinander abhängig sind')\n",
    "\n",
    "#Falls ein Chi-Quadrat Test später nötig sein sollte\n",
    "#erthquakeChi muss vor diesem Test noch weiter bereinigt werden (negative Werte löschen)\n",
    "#print(\"In order to test this hypothesis, we will use a Chi-Square test to proof or counter it\")\n",
    "#newdf = earthquakeChi\n",
    "\n",
    "#to_drop = ['time', 'latitude', 'longitude', 'magType', 'nst', 'gap', 'dmin', 'rms', 'magError', 'magNst', 'status', 'locationSource', 'magSource', 'Region', 'Jahr', 'Monat', 'Tag', 'Total']\n",
    "#newdf.drop(to_drop, inplace=True, axis=1)\n",
    "\n",
    "#to_drop = ['net', 'id', 'updated', 'place', 'type', 'horizontalError', 'depthError']\n",
    "#newdf.drop(to_drop, inplace=True, axis=1)\n",
    "#print(newdf.head())\n",
    "#stat, p, dof, expected = chi2_contingency(newdf)\n",
    "#print(p)\n",
    "#print(dof)\n",
    "# interpret test-statistic\n",
    "#prob = 0.95\n",
    "#critical = stats.chi2.ppf(prob, dof)\n",
    "#if abs(stat) >= critical:\n",
    "#    print('Dependent (reject H0)')\n",
    "#else:\n",
    "#    print('Independent (fail to reject H0)')\n",
    "\n",
    "\n",
    "# interpret p-value\n",
    "#alpha = 1.0 - prob\n",
    "#if p <= alpha:\n",
    "#    print('Dependent (reject H0)')\n",
    "#else:\n",
    "#    print('Independent (fail to reject H0)')\n",
    "\n",
    "#print(\"The test results show us, that the variables are independet from one another\")\n",
    "\n",
    "\n",
    "\n",
    "# Hypthese 2: Die Zahl der Erdbeben nimmt von Jahr zu Jahr zu\n",
    "#Gegenhypothese: Die Zahl der Erbeben bleibt gleich oder nimmt ab\n",
    "#Aufteilen des Dateframes\n",
    "erthquake2019= earthquakeChanged[earthquakeChanged['Jahr'] == 2019]\n",
    "erthquake2019C=erthquake2019.count()\n",
    "erthquake2020= earthquakeChanged[earthquakeChanged['Jahr'] == 2020]\n",
    "erthquake2020C=erthquake2020.count()\n",
    "erthquake2021= earthquakeChanged[earthquakeChanged['Jahr'] == 2021]\n",
    "erthquake2021C=erthquake2021.count()\n",
    "y=[erthquake2019C, erthquake2020C, erthquake2021C]\n",
    "x=[2019, 2020, 2021]\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.title('Erdbeben pro Jahr')\n",
    "plt.xlabel('Jahreszahl')\n",
    "plt.ylabel('Erdbeben pro Jahr')\n",
    "plt.show()\n",
    "\n",
    "print('Es kann mit den vorhandenen Daten kein klarer Trend ausgemacht werden. Weder die Hypothese noch die Gegenhypothese können klar wiederlegt oder bestätigt werden.')\n",
    "\n",
    "# Hypothese 3:Es gibt Orte, an denen sich Erdbeben bestimmter stärke häufen am Beispiel Alaska\n",
    "# Gegenhypothese: Es kommt zu keiner Häufung der Erdbeben bestimmter stärke an einem Ort am Beispiel Alaska\n",
    "\n",
    "\n",
    "earthquakeklein= earthquakeChanged[earthquakeChanged['mag'] < 5]\n",
    "#Anteil kleiner Erdbeben\n",
    "p=earthquakeklein.count()/earthquakeChanged.count()*100\n",
    "print('Anteil der Kleinen Beben an der gesamtzahl der Beben')\n",
    "\n",
    "print(p)\n",
    "\n",
    "earthquakeChanged = earthquakeChanged.astype({\"Region\": str})\n",
    "\n",
    "#Berrechne p für Alaska\n",
    "print(earthquakeChanged['Region'].value_counts())\n",
    "#Daraus folgt, Alaska hatte 13939 Erdbeben zu verzeichnen\n",
    "ErbebenAlaska=13939\n",
    "print(earthquakeklein['Region'].value_counts())\n",
    "#Daraus folgt, Alaska hatte 13805 kleine Erdbeben zu verzeichnen\n",
    "ErbebenAlaskaKlein=13805\n",
    "pAlaska=ErbebenAlaskaKlein/ErbebenAlaska*100\n",
    "\n",
    "print('Anteil der Kleinen Beben an der gesamtzahl der Beben in Alaska')\n",
    "\n",
    "print(pAlaska)\n",
    "\n",
    "print('Darasu lässt sich schließen, das kleine Erdbeben in Alaska öfter vorkommen als im Durchschnitt. Somit wird die Gegenhypothese abgelehnt und Hypothese 3 gilt')\n",
    "# Hypothes4: Wenn es ein Erdbeben gibt gibt es im gleichen Jahr ein weiteres Erdbeben\n",
    "# Ggegnhypothes: Erdbeben kommen nicht nochmal am selben Ort vor\n",
    "\n",
    "print(earthquakeChanged.groupby(['Region', 'Jahr']).size())\n",
    "print('Ergebnise diese Gropu By zeigen eindeutig das es vorkommt, das es im selben Jahr mehrere Erdbeben gibt, Gegenhypothese wird abgelehnt')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aufgabe 1.7 \n",
    "#Provide the conclusion of your earthquake research.\n",
    "# Will be done here Step 4: Show your solution: the Model of Data and any conclutions\n",
    "#Is it possible to perfume here A/B Test? What it can show? What confidence intervals and p-values show us in regard to testing of Null-hypothesis?\n",
    "\n",
    "#Es ist nicht möglich oder sinnvoll, A/B TEsts hier durchzuführen. A/B testing is a basic randomized control experiment. It is a way to compare the two versions of a variable to find out which performs better in a controlled environment.\n",
    "# Bei Erdbeben handelt es sich um natürliche geschehen die nicht in kontrollierbare Umgebung auftreten. Es kann als nicht nur eine Variable verändert werden um zu beobachten, was passiert. Dies geht weder bei den hier verwendetetn Daten noch bei den eigentlich sehr viel komplexeren Daten in der Natur und ist auch nicht sinnvoll\n",
    "\n",
    "#Die Hypothesen werden nicht über die p-Values getestet in meinem Fall, deshalb gibt es auch hierzu keine Aussage\n",
    "\n",
    "\n",
    "# Aufgabe 1.8\n",
    "\n",
    "#In sum, the Bonferroni correction method is a simple way of controlling the Type I error rate in hypothesis testing. \n",
    "#To calculate the new alpha level, simply divide the original alpha by the number of comparisons being made. However, \n",
    "#since this approach is rather strict, it may be more appropriate to use alternative means of controlling for multiple \n",
    "#comparisons.\n",
    "\n",
    "#Type I error, in the context of hypothesis testing, is the likelihood of discovering a false-positive result, \n",
    "#thus rejecting a true null hypothesis\n",
    "\n",
    "#Der p-Wert (nach R. A. Fisher), auch Überschreitungswahrscheinlichkeit oder Signifikanzwert genannt \n",
    "#(p für lateinisch probabilitas = Wahrscheinlichkeit), ist in der Statistik und dort insbesondere in der \n",
    "#Testtheorie ein Evidenzmaß für die Glaubwürdigkeit der Nullhypothese, die oft besagt, dass ein bestimmter Zusammenhang \n",
    "#nicht besteht, z. B. ein neues Medikament nicht wirksam ist.\n",
    "\n",
    "RangeTest=10000\n",
    "earthquakeChanged = earthquakeChanged.astype({\"depth\": float, 'mag': float})\n",
    "DepthArray=earthquakeChanged['depth'].to_numpy()\n",
    "\n",
    "MagArray=earthquakeChanged['mag'].to_numpy()\n",
    "\n",
    "def pearson_r(x, y):\n",
    "    \"\"\"Compute Pearson correlation coefficient between two arrays.\"\"\"\n",
    "    # Compute correlation matrix: corr_mat\n",
    "    corr_mat = np.corrcoef(x, y)\n",
    "\n",
    "    # Return entry [0,1]\n",
    "    return corr_mat[0,1]\n",
    "\n",
    "# Compute Pearson correlation coefficient for I. versicolor\n",
    "r = pearson_r(DepthArray, MagArray)\n",
    "\n",
    "# Print the result\n",
    "print('Pearson Correlation')\n",
    "print(r)\n",
    "\n",
    "# Compute observed correlation: r_obs\n",
    "r_obs = pearson_r(DepthArray, MagArray)\n",
    "\n",
    "# Initialize permutation replicates: perm_replicates\n",
    "perm_replicates = np.empty(RangeTest)\n",
    "\n",
    "# Draw replicates\n",
    "for i in range(RangeTest):\n",
    "    # Permute illiteracy measurments: illiteracy_permuted\n",
    "    DepthArray_permuted = np.random.permutation(DepthArray)\n",
    "\n",
    "    # Compute Pearson correlation\n",
    "    perm_replicates[i] = pearson_r(DepthArray_permuted, MagArray)\n",
    "\n",
    "# Compute p-value: p\n",
    "p = np.sum(perm_replicates >= r_obs) / len(perm_replicates)\n",
    "print('p-val =', p)\n",
    "\n",
    "#Bonferoni Test_\n",
    "#p-Value geteilt durch die Anzahl der Tests\n",
    "Bon=p/RangeTest\n",
    "print('Bonferoni',Bon)\n",
    "\n",
    "\n",
    "earthquakeChanged = earthquakeChanged.astype({\"depth\": float, 'mag': float})\n",
    "DepthArray=earthquakeChanged['depth'].to_numpy()\n",
    "\n",
    "MagArray=earthquakeChanged['mag'].to_numpy()\n",
    "\n",
    "def pearson_r(x, y):\n",
    "    \"\"\"Compute Pearson correlation coefficient between two arrays.\"\"\"\n",
    "    # Compute correlation matrix: corr_mat\n",
    "    corr_mat = np.corrcoef(x, y)\n",
    "\n",
    "    # Return entry [0,1]\n",
    "    return corr_mat[0,1]\n",
    "\n",
    "# Compute Pearson correlation coefficient for I. versicolor\n",
    "r = pearson_r(MagArray, DepthArray)\n",
    "\n",
    "# Print the result\n",
    "print('Pearson Correlation')\n",
    "print(r)\n",
    "\n",
    "# Compute observed correlation: r_obs\n",
    "r_obs = pearson_r(MagArray, DepthArray)\n",
    "\n",
    "# Initialize permutation replicates: perm_replicates\n",
    "perm_replicates = np.empty(RangeTest)\n",
    "\n",
    "# Draw replicates\n",
    "for i in range(RangeTest):\n",
    "    # Permute illiteracy measurments: illiteracy_permuted\n",
    "    MagArray_permuted = np.random.permutation(MagArray)\n",
    "\n",
    "    # Compute Pearson correlation\n",
    "    perm_replicates[i] = pearson_r(MagArray_permuted, DepthArray)\n",
    "\n",
    "# Compute p-value: p\n",
    "p = np.sum(perm_replicates >= r_obs) / len(perm_replicates)\n",
    "print('p-val =', p)\n",
    "\n",
    "#Bonferoni Test\n",
    "#p-Value geteilt durch die Anzahl der Tests\n",
    "Bon=p/RangeTest\n",
    "print('Bonferoni',Bon)\n",
    "print('Die Hypothese, das die Tiefe und die Stärke Korrelieren wird trotz der positiven Pearson Correlation abgelehnt, das sowohl p-Value als auch Bonferoni Korrektur 0 sind.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Show your solution: the Model of  Data and any conclutions \n",
    "#### 4.1 Create graphics, any needed queris to visualise your solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- how likely it's that in places where there have already been earthquakes there will be more earthquakes? What would be the trend in 10 years?\n",
    "- Whether there are special places where earthquakes occur certain strength piling.\n",
    "- A correlation between values such as depth and strength (or other values related to strength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how likely it's that in places where there have already been earthquakes there will be more earthquakes? What would be the trend in 10 years?\n",
    "\n",
    "print('Es wird als gegeben angesehen das Erdbeben innerhaln eines Jahr an einem Ort öfter vorkommen. Dies wird durch diese Auflistung verdeutlicht, in der an mehreren Orten mehrere Erdbeben im gleichen Jahr vorkommen')\n",
    "print(earthquakeChanged.groupby(['Region', 'Jahr']).size())\n",
    "\n",
    "#In ein Dataframe umwandeln\n",
    "earthquakeDataframe=earthquakeChanged.groupby(['Region', 'Jahr']).size().reset_index(name='counts')\n",
    "\n",
    "print(earthquakeDataframe)\n",
    "\n",
    "#Häufigkeit der Erdbeben über 1\n",
    "\n",
    "earthquakeDataframe1=earthquakeDataframe.loc[earthquakeDataframe['counts']>1]\n",
    "\n",
    "print(earthquakeDataframe1)\n",
    "#Vergleich der größe der Dataframe\n",
    "\n",
    "size1=earthquakeDataframe1.size\n",
    "size=earthquakeDataframe.size\n",
    "\n",
    "if(size==size1):\n",
    "    print('In 100% der Fälle traten Erdbeben nicht vereinzelt auf')\n",
    "else:\n",
    "    Häufigkeit=size1/size*100\n",
    "    Häufigkeit=round(Häufigkeit,2)\n",
    "    print('In',Häufigkeit,'  der Fälle traten Erdbeben nicht vereinzelt auf')\n",
    "    HäufigkeitNicht= (size-size1)/size*100\n",
    "    HäufigkeitNicht=round(HäufigkeitNicht,2)\n",
    "    print('In',HäufigkeitNicht,'  der Fälle traten Erdbeben vereinzelt auf')\n",
    "\n",
    "    \n",
    "print('Wie die Graphik zeigt, ist ein Trend nicht feststellbar, weder in den letzten Jahren noch für die Zukunft. HIerfür wären mehr Daten aus den Jahren 2018 und dabor nötig')\n",
    "erthquake2019= earthquakeChanged[earthquakeChanged['Jahr'] == 2019]\n",
    "erthquake2019C=erthquake2019.count()\n",
    "erthquake2020= earthquakeChanged[earthquakeChanged['Jahr'] == 2020]\n",
    "erthquake2020C=erthquake2020.count()\n",
    "erthquake2021= earthquakeChanged[earthquakeChanged['Jahr'] == 2021]\n",
    "erthquake2021C=erthquake2021.count()\n",
    "y=[erthquake2019C, erthquake2020C, erthquake2021C]\n",
    "x=[2019, 2020, 2021]\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.title('Erdbeben pro Jahr')\n",
    "plt.xlabel('Jahreszahl')\n",
    "plt.ylabel('Erdbeben pro Jahr')\n",
    "plt.show()  \n",
    "\n",
    "#Whether there are special places where earthquakes occur certain strength piling.\n",
    "print('Wie das Beispiel Alaska zeigt gibt es Orte, an denen eingie Erdbeben in Bezug auf dei Stärke häufiger sind als im Durchscnitt. Verdeutlicht wird dies durch die folgenden Auflistungen.')\n",
    "\n",
    "earthquakeklein= earthquakeChanged[earthquakeChanged['mag'] < 5]\n",
    "#Anteil kleiner Erdbeben\n",
    "p=earthquakeklein.count()/earthquakeChanged.count()*100\n",
    "print('Anteil der Kleinen Beben an der gesamtzahl der Beben')\n",
    "\n",
    "print(p)\n",
    "\n",
    "earthquakeChanged = earthquakeChanged.astype({\"Region\": str})\n",
    "\n",
    "#Berrechne p für Alaska\n",
    "print(earthquakeChanged['Region'].value_counts())\n",
    "#Daraus folgt, Alaska hatte 13939 Erdbeben zu verzeichnen\n",
    "ErbebenAlaska=13939\n",
    "print(earthquakeklein['Region'].value_counts())\n",
    "#Daraus folgt, Alaska hatte 13805 kleine Erdbeben zu verzeichnen\n",
    "ErbebenAlaskaKlein=13805\n",
    "pAlaska=ErbebenAlaskaKlein/ErbebenAlaska*100\n",
    "\n",
    "print('Anteil der Kleinen Beben an der gesamtzahl der Beben in Alaska')\n",
    "\n",
    "print(pAlaska)\n",
    "\n",
    "\n",
    "#A correlation between values such as depth and strength (or other values related to strength)\n",
    "print('Die Pearson Correlation zeigt, das es zwischen diesen Werten einen zusammenhang gibt. Zu sehen in dieser Heatmap und dem Pearson Coeffizient.')\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title('Heatmap Correlation numerical values')\n",
    "sns.heatmap(earthquakeChanged.corr(),cbar=True,annot=True,cmap='Blues')\n",
    "plt.show()\n",
    "print('Pearson Correlation')\n",
    "r = pearson_r(earthquakeChanged['mag'], earthquakeChanged['depth'])\n",
    "print('Die p-Values wiederlegen allerdings diese Aussage und zeigen, das es keine Correlation gibt')\n",
    "p = np.sum(perm_replicates >= r_obs) / len(perm_replicates)\n",
    "print('p-val =', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the dataframes. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform quality checks here\n",
    "#Die oben gennanten Ansätze machen im Bezug auf diese Daten nur wenig sinn, daher werden hier einige eigene Ansätze vorgestellt\n",
    "\n",
    "#Check ob CSV's gleiche Spalten haben\n",
    "newlist1 = []\n",
    "for col in earthquake1.columns:\n",
    "     newlist1.append(col)\n",
    "\n",
    "newlist2 = []\n",
    "for col in earthquake2.columns:\n",
    "     newlist2.append(col)\n",
    "        \n",
    "newlist3 = []\n",
    "for col in earthquake3.columns:\n",
    "     newlist3.append(col)\n",
    "\n",
    "newlist4 = []\n",
    "for col in earthquake4.columns:\n",
    "     newlist4.append(col)        \n",
    "\n",
    "newlist5 = []\n",
    "for col in earthquake5.columns:\n",
    "     newlist5.append(col)        \n",
    "        \n",
    "if(newlist1==newlist2):\n",
    "    if(newlist2==newlist3):\n",
    "        if(newlist3==newlist4):\n",
    "            if(newlist4==newlist5):\n",
    "                print('Die CSV haben den gleichen Header')\n",
    "else:\n",
    "    print('Header sind nicht gleich')\n",
    "    \n",
    "earthquakeBackup = earthquakeBackup.astype({\"magError\": float, 'magNst': float})\n",
    "#Nutzen der Error Daten aus earthquakeBackup\n",
    "#magError\n",
    "meanmagError=np.mean(earthquakeBackup['magError'])\n",
    "#Alles was über dem mittelwert +-50% Mittelwert liegt rausrechnen\n",
    "meanmagErrorMinus=meanmagError-0.5*meanmagError\n",
    "meanmagErrorPlus=meanmagError+0.5*meanmagError\n",
    "\n",
    "earthquakemeanMag=earthquakeBackup.loc[earthquakeBackup['magError']<meanmagErrorPlus]\n",
    "earthquakemeanMag=earthquakemeanMag.loc[earthquakemeanMag['magError']>meanmagErrorMinus]\n",
    "CountBackup=earthquakeBackup.size\n",
    "CountmeanMag=earthquakemeanMag.size\n",
    "\n",
    "#Prozentuale angabe der gelöschten Daten\n",
    "PGD=(CountBackup-CountmeanMag)/CountBackup *100\n",
    "PGD=round(PGD, 2)\n",
    "print('Gelöschte Daten in %:', PGD )\n",
    "\n",
    "\n",
    "#Nutzen der Error Daten aus earthquakeBackup\n",
    "#magNST\n",
    "meanmagNST=np.mean(earthquakemeanMag['magNst'])\n",
    "#Alles was über dem mittelwert +-50% Mittelwert liegt rausrechnen\n",
    "meanmagNSTMinus=meanmagNST-0.5*meanmagNST\n",
    "meanmagNSTPlus=meanmagNST+0.5*meanmagNST\n",
    "\n",
    "earthquakemeanNST=earthquakemeanMag.loc[earthquakemeanMag['magNst']<meanmagNSTPlus]\n",
    "earthquakemeanNST=earthquakemeanNST.loc[earthquakemeanNST['magNst']>meanmagNSTMinus]\n",
    "CountBackup=earthquakeBackup.size\n",
    "CountmeanNST=earthquakemeanNST.size\n",
    "\n",
    "#Prozentuale angabe der gelöschten Daten\n",
    "PGD=(CountBackup-CountmeanMag)/CountBackup *100\n",
    "PGD=round(PGD, 2)\n",
    "print('Es sollten:', PGD, '% Daten nur gesondert berücksichtigt werden' )\n",
    "#In dieser Arbeit wurden alle DAten berücksichtig\n",
    "\n",
    "\n",
    "#Tiefen kleiner 0 (liegen in der Luft) nicht berücksichtigen\n",
    "earthquakeChangedDepth=earthquakeChanged.loc[earthquakeChanged['depth']<0]\n",
    "positiveTiefe=earthquakeChangedDepth.size\n",
    "print(positiveTiefe, ' Daten sollten nicht berücksichtigt werden')\n",
    "\n",
    "#Weitere Möglichkeiten wäre das normalisieren von Daten(z.B. der Region) und das gezielte umwandeln von Datentypen um die Konsistenz der Werte zu überprüfen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies, algorithms, methodes for the project.\n",
    "* Propose how often the data should be updated and why. What tools you are used for data simulation in case if it is needed.\n",
    "* Write a description of how you would approach the problem differently if you have another data model or additinal information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.3 Data dictionary\n",
    "\n",
    "# Information about this topics: https://earthquake.usgs.gov/data/comcat/data-eventterms.php\n",
    "\n",
    "#Hier sind die in diesem File eingebundenen Daten\n",
    "\n",
    "#time: Wann ist das Erdbeben passiert\n",
    "#latidude: Geographische Koordinate\n",
    "#longitude: Geographische Koordinate\n",
    "#depth: Tiefe des Erdbeben\n",
    "#mag: Stärke auf Richterskala\n",
    "#magType: Magnitude Types (More Infos: https://www.usgs.gov/natural-hazards/earthquake-hazards/science/magnitude-types?qt-science_center_objects=0#qt-science_center_objects)\n",
    "#nst: Number of seismic stations which reported P- and S-arrival times for this earthquake\n",
    "#gap: azimuthal gap\n",
    "#dmin: Horizontal distance from the epicenter to the nearest station\n",
    "#rms: root-mean-squared residual of solution\n",
    "#net: The ID of a data contributor\n",
    "#id: A unique identifier for the event\n",
    "#updated: time when the event was most recently updated\n",
    "#place: Textual description of named geographic region near to the event\n",
    "#type: Type of seismic event\n",
    "#horizontalError: he horizontal location error, in km, defined as the length of the largest projection of the three principal errors on a horizontal plane\n",
    "#depthError: The depth error, in km, defined as the largest projection of the three principal errors on a vertical line\n",
    "#magError: Uncertainty of reported magnitude of the event\n",
    "#magNST: The total number of seismic stations used to calculate the magnitude for this earthquake\n",
    "#status: Status is either automatic or reviewed\n",
    "#locationSource: Decimal degrees longitude. Negative values for western longitudes\n",
    "#magSource: The method or algorithm used to calculate the preferred magnitude for the event\n",
    "\n",
    "\n",
    "#Eine vollständige Liste mit genauerer Beschreibung finden Sie in der beigelegten PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5: Complete Project Write Up\n",
    "\n",
    "#Clearly state the rationale for the choice of tools and technologies, algorithms, methodes for the project.\n",
    "# Python: Durch libs gut für Data Science geeignet\n",
    "# JupyterLab:  web-based interactive development environment for Jupyter notebooks\n",
    "# pandas as pd: used to analyze data\n",
    "# matplotlib.pyplot as plt: used to plot data\n",
    "# seaborn as sns: provides a high-level interface for drawing attractive and informative statistical graphics.\n",
    "# numpy as np: The fundamental package for scientific computing with Python\n",
    "# statistics: module provides functions for calculating mathematical statistics of numeric ( Real -valued) data.\n",
    "# warnings: ignore warnigs that don´t affect execution of code\n",
    "# sympy import S, symbols, printing: Python library for symbolic mathematics\n",
    "# statsmodels.sandbox.stats.multicomp import multipletests: lib für Bonferoni Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Propose how often the data should be updated and why. What tools you are used for data simulation in case if it is needed.\n",
    "#Data muss nicht geupdated werden, da die Werte absolut sind und sich nur selten ändern. Aus diesem Grund empfehle ich die Daten \n",
    "#einmal im Jahr über ein allgorithmus zu updaten, bei dem nur die Daten die geändert wurden berücksichtigt werden.\n",
    "\n",
    "# Wichtiger ist ein System zu nehmen, welches mehr Daten gleichzeitig ohne größeren Zeitverlust aufnehmen und analysieren kann, um dadurch durch Daten früherer Jahre einen\n",
    "# Trend für die Zukunft zu finden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a description of how you would approach the problem differently if you have another data model or additinal information\n",
    "\n",
    "#Zu erst würde ich bei einem erneuten Durchführen dieser Aufgabe mehr auf \"ordentlichen\" Python-Code achten und die jeweiligen Abläufe in diesem Notebook in Funktionen aufteilen (Zeitmangel in diesem Projekt führte zu dieser teilweise unübersichtlichen Darstellung).\n",
    "\n",
    "# Weiterhin wäre ein freierer Ansatz sinnvoll da die meisten Hypothesen sich bereits früh als Falsch oder mit den Daten nicht untersuchbar herrausgestellt haben. \n",
    "# Selbiges gilt für geforderte Tests die in diesem Umfeld nur wenig sinn machen\n",
    "# Eine gründlichere Bereinigung der Daten und ein besseres Konzept am Anfang (speziell in BEzug auf die Datentypen) würde viel Zeit sparen für Zukünftige Projekte\n",
    "\n",
    "# Wie bereits erwähnt wäre zusätzliche DAten bezüglich der Jahre 2018 und früher sinnvoll um weitere Fragen abzudecken. Außerdem wären genauere Daten zu Eigenschaften wie dem Gesteinstyp sicher sinnvoll um bessere Correlationen zu machen.\n",
    "# Das einlesen eines Länderkatalogs (Welches Gebiet gehört zu welchem Land) würde ein Leichteres Gruppieren der DAten ermöglichen und neue/aussagekräftigere Analysen der Daten ermöglichen.\n",
    "# Daten zu Zyklen wie dem Jahreszeitzyklus, Sonne/Mond Zyklus etc. könnte weitere Zusammenhänge aufdecken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
